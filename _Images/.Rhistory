return(exp(-((x-y)^2)/(sigma^2)))
}
#Generating Random Variables
M<-10000
U1<-runif(M,(-pi/2),pi/2)
U2<-runif(M,(-pi/2),pi/2)
W1<-rexp(M,rate=1)
W2<-rexp(M,rate=1)
#Defining alpha-simulator
simulator<-function(U,W,theta){
alpha<-theta[1]
beta<-theta[2]
gamma<-theta[3]
delta<-theta[4]
zeta<- (-beta)*tan((pi*alpha)/2)
xi<- (1/alpha)*atan(-zeta)
first<-(1+zeta^2)^(1/(2*alpha))
second<-(sin(alpha*(U+xi)))/(cos(U)^(1/alpha))
third<-((cos(U-alpha*(U+xi)))/(W))^((1-alpha)/(alpha))
X<-first*second*third
return(gamma*X+delta)
}
#Defining MMD estimate
MMDist<-function(theta){
sum1<-0
for (i in (1:M)){
part1<-kern(simulator(U1[i],W1[i],theta),simulator(U2[i],W2[i],theta))
sum2<-0
for (j in (1:N)){
sum2<-sum2+kern(simulator(U1[i],W1[i],theta),obs.data[j])
}
part2<-(2/N)*sum2
sum1<-sum1+part1-part2
}
return((1/M)*sum1)
}
#Will this work?
theta.init<-c(1.5,0.5,1.1,0.1)
theta.true<-c(1.8,0.7,1.2,0.5)
#optimiser<-optim(theta.init,MMDist,method="L-BFGS-B",lower=c(1,0,0,0),upper=c(2,1,2,2))
#theta.maybe<-optimiser$par
MMDist(theta.init)
MMDist(theta.true)
#optimiser<-optim(theta.init,MMDist,method="L-BFGS-B",lower=c(1,0,0,0),upper=c(2,1,2,2))
#theta.maybe<-optimiser$par
MMDist(theta.init)
knitr::opts_chunk$set(echo = TRUE)
##Re-attempting the indirect inference example using alpha-stable dist
set.seed(123)
library(stabledist)
library(stats4)
library(EnvStats)
#We set our 'true values' to alpha=1.8 and beta=0.7, gamma and delta are default 1 and 0
N=1000
obs.data<-rstable(N,1.8,0.7,1.2,0.5)
#obs.data2<-rstable(N,theta.maybe[1],theta.maybe[2],theta.maybe[3],theta.maybe[4])
#test<-quantile(obs.data,probs=c(0.01,0.99))
#test=as.numeric(test)
#obs.data<-obs.data[test[1]<obs.data & obs.data<test[2]]
#plot(density(obs.data2))
#N<-length(obs.data)
plot(density(obs.data))
#lines(density(obs.data2),col='red')
#Defining kernels
#Gaussian Kernel
sigma<-0.5
kern<-function(x,y){
return(exp(-((x-y)^2)/(sigma^2)))
}
#Generating Random Variables
M<-1000
U1<-runif(M,(-pi/2),pi/2)
U2<-runif(M,(-pi/2),pi/2)
W1<-rexp(M,rate=1)
W2<-rexp(M,rate=1)
#Defining alpha-simulator
simulator<-function(U,W,theta){
alpha<-theta[1]
beta<-theta[2]
gamma<-theta[3]
delta<-theta[4]
zeta<- (-beta)*tan((pi*alpha)/2)
xi<- (1/alpha)*atan(-zeta)
first<-(1+zeta^2)^(1/(2*alpha))
second<-(sin(alpha*(U+xi)))/(cos(U)^(1/alpha))
third<-((cos(U-alpha*(U+xi)))/(W))^((1-alpha)/(alpha))
X<-first*second*third
return(gamma*X+delta)
}
#Defining MMD estimate
MMDist<-function(theta){
sum1<-0
for (i in (1:M)){
part1<-kern(simulator(U1[i],W1[i],theta),simulator(U2[i],W2[i],theta))
sum2<-0
for (j in (1:N)){
sum2<-sum2+kern(simulator(U1[i],W1[i],theta),obs.data[j])
}
part2<-(2/N)*sum2
sum1<-sum1+part1-part2
}
return((1/M)*sum1)
}
#Will this work?
theta.init<-c(1.5,0.5,1.1,0.1)
theta.true<-c(1.8,0.7,1.2,0.5)
#optimiser<-optim(theta.init,MMDist,method="L-BFGS-B",lower=c(1,0,0,0),upper=c(2,1,2,2))
#theta.maybe<-optimiser$par
MMDist(theta.init)
MMDist(theta.true)
#MMDist(theta.maybe)
#test<-kern(simulator(U1,W,theta.init),simulator(U2,W,theta.init))
#test3<-kern(simulator(U1,W,theta.init),obs.data)
#test1<-simulator(U1,W,theta.init)
#test2<-simulator(U2,W,theta.init)
#hist(test)
test1=rep(0,10)
test2=rep(0,10)
test3=rep(0,10)
test4=rep(0,10)
xs1=seq(1.01,2,0.1)
xs2=seq(0.01,1,0.1)
xs3=seq(0.01,2,0.2)
xs4=seq(0.01,2,0.2)
for (i in (1:10)){
test1[i]=MMDist(c(xs1[i],0.7,1.2,0.5))
test2[i]=MMDist(c(1.8,xs2[i],1.2,0.5))
test3[i]=MMDist(c(1.8,0.7,xs3[i],0.5))
test4[i]=MMDist(c(1.8,0.7,1.2,xs4[i]))
}
par(mfrow=c(2,2))
plot(xs1,test1)
plot(xs2,test2)
plot(xs3,test3)
plot(xs4,test4)
knitr::opts_chunk$set(echo = TRUE)
##Re-attempting the indirect inference example using alpha-stable dist
set.seed(123)
library(stabledist)
library(stats4)
library(EnvStats)
#We set our 'true values' to alpha=1.8 and beta=0.7, gamma and delta are default 1 and 0
N=1000
obs.data<-rstable(N,1.8,0.7,1.2,0.5)
#obs.data2<-rstable(N,theta.maybe[1],theta.maybe[2],theta.maybe[3],theta.maybe[4])
#test<-quantile(obs.data,probs=c(0.01,0.99))
#test=as.numeric(test)
#obs.data<-obs.data[test[1]<obs.data & obs.data<test[2]]
#plot(density(obs.data2))
#N<-length(obs.data)
plot(density(obs.data))
#lines(density(obs.data2),col='red')
#Defining kernels
#Gaussian Kernel
sigma<-0.5
kern<-function(x,y){
return(exp(-((x-y)^2)/(sigma^2)))
}
#Generating Random Variables
M<-1000
U1<-runif(M,(-pi/2),pi/2)
U2<-runif(M,(-pi/2),pi/2)
W1<-rexp(M,rate=1)
W2<-rexp(M,rate=1)
#Defining alpha-simulator
simulator<-function(U,W,theta){
alpha<-theta[1]
beta<-theta[2]
gamma<-theta[3]
delta<-theta[4]
zeta<- (-beta)*tan((pi*alpha)/2)
xi<- (1/alpha)*atan(-zeta)
first<-(1+zeta^2)^(1/(2*alpha))
second<-(sin(alpha*(U+xi)))/(cos(U)^(1/alpha))
third<-((cos(U-alpha*(U+xi)))/(W))^((1-alpha)/(alpha))
X<-first*second*third
return(gamma*X+delta)
}
#Defining MMD estimate
MMDist<-function(theta){
sum1<-0
for (i in (1:M)){
part1<-kern(simulator(U1[i],W1[i],theta),simulator(U2[i],W2[i],theta))
sum2<-0
for (j in (1:N)){
sum2<-sum2+kern(simulator(U1[i],W1[i],theta),obs.data[j])
}
part2<-(2/N)*sum2
sum1<-sum1+part1-part2
}
return((1/M)*sum1)
}
#Will this work?
theta.init<-c(1.5,0.5,1.1,0.1)
theta.true<-c(1.8,0.7,1.2,0.5)
#optimiser<-optim(theta.init,MMDist,method="L-BFGS-B",lower=c(1,0,0,0),upper=c(2,1,2,2))
#theta.maybe<-optimiser$par
MMDist(theta.init)
MMDist(theta.true)
#MMDist(theta.maybe)
#test<-kern(simulator(U1,W,theta.init),simulator(U2,W,theta.init))
#test3<-kern(simulator(U1,W,theta.init),obs.data)
#test1<-simulator(U1,W,theta.init)
#test2<-simulator(U2,W,theta.init)
#hist(test)
test1=rep(0,10)
test2=rep(0,10)
test3=rep(0,10)
test4=rep(0,10)
xs1=seq(1.01,2,0.1)
xs2=seq(0.01,1,0.1)
xs3=seq(0.01,2,0.2)
xs4=seq(0.01,2,0.2)
for (i in (1:10)){
test1[i]=MMDist(c(xs1[i],0.7,1.2,0.5))
test2[i]=MMDist(c(1.8,xs2[i],1.2,0.5))
test3[i]=MMDist(c(1.8,0.7,xs3[i],0.5))
test4[i]=MMDist(c(1.8,0.7,1.2,xs4[i]))
}
par(mfrow=c(2,2))
plot(xs1,test1)
plot(xs2,test2)
plot(xs3,test3)
plot(xs4,test4)
#Defining kernels
#Gaussian Kernel
sigma<-0.05
kern<-function(x,y){
return(exp(-((x-y)^2)/(sigma^2)))
}
#Will this work?
theta.init<-c(1.5,0.5,1.1,0.1)
theta.true<-c(1.8,0.7,1.2,0.5)
#optimiser<-optim(theta.init,MMDist,method="L-BFGS-B",lower=c(1,0,0,0),upper=c(2,1,2,2))
#theta.maybe<-optimiser$par
MMDist(theta.init)
MMDist(theta.true)
#MMDist(theta.maybe)
#Defining kernels
#Gaussian Kernel
sigma<-100
kern<-function(x,y){
return(exp(-((x-y)^2)/(sigma^2)))
}
#Will this work?
theta.init<-c(1.5,0.5,1.1,0.1)
theta.true<-c(1.8,0.7,1.2,0.5)
#optimiser<-optim(theta.init,MMDist,method="L-BFGS-B",lower=c(1,0,0,0),upper=c(2,1,2,2))
#theta.maybe<-optimiser$par
MMDist(theta.init)
MMDist(theta.true)
knitr::opts_chunk$set(echo = TRUE)
##Re-attempting the indirect inference example using alpha-stable dist
set.seed(123)
library(stabledist)
library(stats4)
library(EnvStats)
#We set our 'true values' to alpha=1.8 and beta=0.7, gamma and delta are default 1 and 0
N=1000
obs.data<-rstable(N,1.8,0.7,1.2,0.5)
#obs.data2<-rstable(N,theta.maybe[1],theta.maybe[2],theta.maybe[3],theta.maybe[4])
#test<-quantile(obs.data,probs=c(0.01,0.99))
#test=as.numeric(test)
#obs.data<-obs.data[test[1]<obs.data & obs.data<test[2]]
#plot(density(obs.data2))
#N<-length(obs.data)
plot(density(obs.data))
#lines(density(obs.data2),col='red')
#Defining kernels
#Gaussian Kernel
sigma<-100
kern<-function(x,y){
return(exp(-((x-y)^2)/(sigma^2)))
}
#Generating Random Variables
M<-1000
U1<-runif(M,(-pi/2),pi/2)
U2<-runif(M,(-pi/2),pi/2)
W1<-rexp(M,rate=1)
W2<-rexp(M,rate=1)
#Defining alpha-simulator
simulator<-function(U,W,theta){
alpha<-theta[1]
beta<-theta[2]
gamma<-theta[3]
delta<-theta[4]
zeta<- (-beta)*tan((pi*alpha)/2)
xi<- (1/alpha)*atan(-zeta)
first<-(1+zeta^2)^(1/(2*alpha))
second<-(sin(alpha*(U+xi)))/(cos(U)^(1/alpha))
third<-((cos(U-alpha*(U+xi)))/(W))^((1-alpha)/(alpha))
X<-first*second*third
return(gamma*X+delta)
}
#Defining MMD estimate
MMDist<-function(theta){
sum1<-0
for (i in (1:M)){
part1<-kern(simulator(U1[i],W1[i],theta),simulator(U2[i],W2[i],theta))
sum2<-0
for (j in (1:N)){
sum2<-sum2+kern(simulator(U1[i],W1[i],theta),obs.data[j])
}
part2<-(2/N)*sum2
sum1<-sum1+part1-part2
}
return((1/M)*sum1)
}
#Will this work?
theta.init<-c(1.5,0.5,1.1,0.1)
theta.true<-c(1.8,0.7,1.2,0.5)
#optimiser<-optim(theta.init,MMDist,method="L-BFGS-B",lower=c(1,0,0,0),upper=c(2,1,2,2))
#theta.maybe<-optimiser$par
MMDist(theta.init)
MMDist(theta.true)
#MMDist(theta.maybe)
#Defining kernels
#Gaussian Kernel
sigma<-50
kern<-function(x,y){
return(exp(-((x-y)^2)/(sigma^2)))
}
#Will this work?
theta.init<-c(1.5,0.5,1.1,0.1)
theta.true<-c(1.8,0.7,1.2,0.5)
#optimiser<-optim(theta.init,MMDist,method="L-BFGS-B",lower=c(1,0,0,0),upper=c(2,1,2,2))
#theta.maybe<-optimiser$par
MMDist(theta.init)
MMDist(theta.true)
#MMDist(theta.maybe)
#Will this work?
theta.init<-c(1.5,0.5,1.1,0.1)
theta.true<-c(1.8,0.7,1.2,0.5)
optimiser<-optim(theta.init,MMDist,method="L-BFGS-B",lower=c(1,0,0,0),upper=c(2,1,2,2))
theta.maybe<-optimiser$par
knitr::opts_chunk$set(echo = TRUE)
pic<-readJPEG('C:\\Users\\Danie\\OneDrive\\Documents\\_Uni\\Maths\\Year 4\\Project\\_Images\\trun_farm.jpeg')
set.seed(123)
library(ggplot2)
library(jpeg)
pic<-readJPEG('C:\\Users\\Danie\\OneDrive\\Documents\\_Uni\\Maths\\Year 4\\Project\\_Images\\trun_farm.jpeg')
library(MASS)
N<-500
data<-matrix(data=NA,ncol=2,nrow=N)
for (i in (1:N)){
data[i,]<-mvrnorm(1,c(-0.5,-0.1),Sigma=matrix(data=c(0.1,0.05,0.05,0.15),nrow=2))
}
datas<-as.data.frame(data,col.names=names(data),row.names = names(data))
plot(data[,1],data[,2],xlim=c(-2,2),ylim=c(-2,2))
ggplot(datas, aes(x = V1, y = V2)) +
background_image(pic) +
geom_point(color='yellow') +
xlim(-2,2) +
ylim(-2,2) +
theme(axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank())
library(ggpubr)
install.packages('ggpubr')
install.packages("ggpubr")
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
library(ggpubr)
library(jpeg)
ggplot(datas, aes(x = V1, y = V2)) +
background_image(pic) +
geom_point(color='yellow') +
xlim(-2,2) +
ylim(-2,2) +
theme(axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank())
new_data<-matrix(NA,nrow=N,ncol=2)
for (i in (1:N)){
val<-data[i,]
x<-val[1]
y<-val[2]
if (y<0.9){
if (y>(-4*x-2.6)){
if (y>(0.3*x-0.7)){
new_data[i,]<-data[i,]
}
}
}
}
new_datas<-as.data.frame(new_data)
new_datas<-drop_na(new_datas)
library(tidyr)
new_datas<-as.data.frame(new_data)
new_datas<-drop_na(new_datas)
new_pic<-readJPEG('C\Users\\Danie\\OneDrive\\Documents\\_Uni\\Maths\\Year 4\\Project\\_Images\\Farm_Example_Highlight.jpeg')
new_pic<-readJPEG('C\\Users\\Danie\\OneDrive\\Documents\\_Uni\\Maths\\Year 4\\Project\\_Images\\Farm_Example_Highlight.jpeg')
new_pic<-readJPEG('C\\Users\\Danie\\OneDrive\\Documents\\_Uni\\Maths\\Year 4\\Project\\_Images\\Farm_Example_Highlight.jpeg')
ggplot(new_datas, aes(x = V1, y = V2)) +
background_image(new_pic) +
geom_point(color='yellow') +
xlim(-2,2) +
ylim(-2,2) +
theme(axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank())
new_pic<-readJPEG('C\\Users\\Danie\\OneDrive\\Documents\\_Uni\\Maths\\Year 4\\Project\\_Images\\Farm_Example_Highlight.jpeg')
new_pic<-readJPEG('C\Users\\Danie\\OneDrive\\Documents\\_Uni\\Maths\\Year 4\\Project\\_Images\\Farm_Example_Highlight.jpeg')
new_pic<-readJPEG('C\\Users\\Danie\\OneDrive\\Documents\\_Uni\\Maths\\Year 4\\Project\\_Images\\Farm_Example_Highlight.jpeg')
setwd("C:/Users/Danie/OneDrive/Documents/_Uni/Maths/Year 4/Project/_Images")
new_pic<-readJPEG('Farm_Example_Highlight.jpeg')
set.seed(123)
library(ggpubr)
library(jpeg)
pic<-readJPEG('C:\\Users\\Danie\\OneDrive\\Documents\\_Uni\\Maths\\Year 4\\Project\\_Images\\trun_farm.jpeg')
library(MASS)
N<-500
data<-matrix(data=NA,ncol=2,nrow=N)
for (i in (1:N)){
data[i,]<-mvrnorm(1,c(-0.5,-0.1),Sigma=matrix(data=c(0.1,0.05,0.05,0.15),nrow=2))
}
datas<-as.data.frame(data,col.names=names(data),row.names = names(data))
ggplot(datas, aes(x = V1, y = V2)) +
background_image(pic) +
geom_point(color='yellow') +
xlim(-2,2) +
ylim(-2,2) +
theme(axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank())
new_data<-matrix(NA,nrow=N,ncol=2)
for (i in (1:N)){
val<-data[i,]
x<-val[1]
y<-val[2]
if (y<0.9){
if (y>(-4*x-2.6)){
if (y>(0.3*x-0.7)){
new_data[i,]<-data[i,]
}
}
}
}
library(tidyr)
new_datas<-as.data.frame(new_data)
new_datas<-drop_na(new_datas)
new_pic<-readJPEG('Farm_Example_Highlight.jpeg')
ggplot(new_datas, aes(x = V1, y = V2)) +
background_image(new_pic) +
geom_point(color='yellow') +
xlim(-2,2) +
ylim(-2,2) +
theme(axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank())
new_pic<-readJPEG('Farm_Example_Highlight.jpg')
new_pic<-readJPEG('C:\Users\Danie\OneDrive\Documents\_Uni\Maths\Year 4\Project\_Images\Farm_Example_Highlight.jpeg')
new_pic<-readJPEG('C:\\Users\Danie\OneDrive\Documents\_Uni\Maths\Year 4\Project\_Images\Farm_Example_Highlight.jpeg')
new_pic<-readJPEG('C:\\Users\\Danie\\OneDrive\\Documents\\_Uni\\Maths\\Year 4\\Project\\_Images\\Farm_Example_Highlight.jpeg')
ggplot(new_datas, aes(x = V1, y = V2)) +
background_image(new_pic) +
geom_point(color='yellow') +
xlim(-2,2) +
ylim(-2,2) +
theme(axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank())
knitr::opts_chunk$set(echo = TRUE)
#TRYING METHOD FROM Trunc2 IN ONE DIMENSION
transformer<-function(x,a,b,mu,sig){
return(qnorm(pnorm(x,mu,sig)*(pnorm(b,mu,sig)-pnorm(a,mu,sig))+pnorm(a,mu,sig),mu,sig))
}
#TRYING GIBBS SAMPLER IN TWO DIMENSIONS
#plot(data)
mu<-c(-0.5,-0.1)
Sigma<-matrix(c(0.1,0.05,0.05,0.15),nrow=2)
#GIBBS SAMPLER
#Create an empty matrix
gibbs_datas<-matrix(data=NA,nrow=N/4,ncol=2)
#Set initial value
gibbs_datas[1,]<-c(-.6,-.1)
for (i in 1:(N/4-1)){
#Sample X1(t+1) from F(.|X2(t))
mu.1<-mu[1]+(Sigma[1,2]/Sigma[2,2])*(gibbs_datas[i,2]-mu[2])
sig.1<-sqrt(Sigma[1,1]-((Sigma[1,2]^2)/Sigma[2,2]))
x1<-rnorm(1,mu.1,sig.1)
x1<-transformer(x1,-2,-.5,mu.1,sig.1)
#Sample X2(t+1) from F(.|X1(t+1))
mu.2<-mu[2]+(Sigma[1,2]/Sigma[2,2])*(x1-mu[1])
sig.2<-sqrt(Sigma[2,2]-((Sigma[1,2]^2)/Sigma[1,1]))
x2<-rnorm(1,mu.2,sig.1)
x2<-transformer(x2,-2,0,mu.2,sig.2)
gibbs_datas[i+1,]<-c(x1,x2)
}
plot(data,col='green')
points(gibbs_datas,col='red')
segments(-.5,0,-.5,-100)
segments(-100,0,-.5,0)
#HOORAY!!!
